{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisherman Game - Reinforcement learning model\n",
    "Natalia Vélez, September 1, 2016\n",
    "\n",
    "Loading dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "import itertools\n",
    "import time\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions & utilities\n",
    "\n",
    "First, we'll define some helper functions. The functions defined in the chunk below allow us to:\n",
    "\n",
    "1. Enumerate all of the possible actions, for some number of fishermen (`actionSpace`)\n",
    "2. Compute the payoff for a particular configuration of actions (`getReward`)\n",
    "3. Compute the expected reward of all possible actions (`getAllRewards` => `getExpectedRewards`)\n",
    "4. Convert expected rewards into probabilities, using a softmax rule (`softMax`)\n",
    "5. Marginalize over the probability distribution provided by `softMax`, to update our beliefs about each agents' actions (`marginalize`)—this will be particularly important for recursive reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def actionSpace(n):\n",
    "    return list(itertools.product(['tree', 'fish'], repeat = n))\n",
    "\n",
    "def getReward(state, actions):\n",
    "    fishermen = zip(state['strengths'], actions)\n",
    "    \n",
    "    fish_caught = sum([f[0] for f in fishermen if f[1] == 'fish'])\n",
    "    trees_cleared = sum([f[0] for f in fishermen if f[1] == 'tree'])\n",
    "    \n",
    "    return fish_caught if trees_cleared >= state['trees'] else 0\n",
    "\n",
    "def getAllRewards(state):\n",
    "    '''\n",
    "    NB: Because players have to try again if they reach a non-optimal solution, all payoffs are either optimal or 0\n",
    "    '''\n",
    "    \n",
    "    # Get all possible actions\n",
    "    n = len(state['strengths'])\n",
    "    allActions = actionSpace(n)\n",
    "    \n",
    "    # Get rewards for each action\n",
    "    allRewards = [getReward(state, a) for a in allActions]\n",
    "    allRewards = [r if r == max(allRewards) else 0 for r in allRewards]\n",
    "    \n",
    "    # Return all action-payoff pairings\n",
    "    return allRewards\n",
    "\n",
    "def getExpectedReward(state, beliefs):\n",
    "    # Get all possible actions and associated rewards\n",
    "    n = len(state['strengths'])\n",
    "    allActions = actionSpace(n)\n",
    "    allRewards = getAllRewards(state)\n",
    "    \n",
    "    # Get probability of each set of actions in actionSpace\n",
    "    singleAction = lambda a, p: p if a == 'tree' else 1-p\n",
    "    actionProb = lambda actions, p: np.prod([singleAction(a, p) for a,p in zip(actions, beliefs)])\n",
    "    allProbs = [actionProb(actions, beliefs) for actions in allActions]\n",
    "    \n",
    "    return zip(allActions, np.multiply(allRewards, allProbs))\n",
    "\n",
    "def softMax(expectedRewards, temp):\n",
    "    actions, utilities = zip(*expectedRewards)\n",
    "    weightedUtilities = np.multiply(utilities, temp)\n",
    "    return zip(actions, weightedUtilities/np.sum(weightedUtilities))\n",
    "\n",
    "def marginalize(beliefs, actionProbs):\n",
    "    marginals = []\n",
    "    n = len(actionProbs[0][0])\n",
    "    \n",
    "    for agent in range(n):\n",
    "        # 'tree_worlds' are all possible scenarios in which the agent chooses to clear trees, etc.\n",
    "        tree_worlds = np.multiply(beliefs[agent], [p for a, p in actionProbs if a[agent] == 'tree'])\n",
    "        fish_worlds = np.multiply(1-beliefs[agent], [p for a, p in actionProbs if a[agent] == 'fish'])\n",
    "        \n",
    "        marginals.append(sum(tree_worlds)/sum(tree_worlds + fish_worlds))\n",
    "    \n",
    "    return marginals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action selection\n",
    "\n",
    "Now that the foundations for our model are set up, let's make some predictions! The methods below:\n",
    "\n",
    "1. Compute the expected reward and softmax action probabilities of each permutation of actions at depth k (`actionPrediction`)\n",
    "2. Use the output of `actionPrediction` to sample a single agent's actions (`agentModel`)\n",
    "3. Sample actions from all agents (`combinedModel`)—this will be particularly important to run simulations, below, as we assume that each agent is choosing their actions independently, based on their beliefs about the group's likely actions.\n",
    "4. Update the groups' beliefs based on the outcome of a turn, i.e, the output of `combinedModel` (`updateBeliefs`)—this part of the code is the most likely to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns probability of all combinations of actions\n",
    "def actionPrediction(state, beliefs, depth, temp):\n",
    "    # Get the probability of each outcome\n",
    "    expectedRewards = getExpectedReward(state, beliefs)\n",
    "    actionProbs = softMax(expectedRewards, temp)\n",
    "    \n",
    "    # Reason recursively about other agents, and get probability of outcomes\n",
    "    if depth > 0:\n",
    "        beliefs = marginalize(beliefs, actionProbs)\n",
    "        actionProbs = actionPrediction(state, beliefs, depth-1, temp)\n",
    "    \n",
    "    return actionProbs\n",
    "\n",
    "# Samples the actions of a single agent\n",
    "def agentModel(state, beliefs, depth = 2, agent = 0, temp = 1):\n",
    "    actionProbs = actionPrediction(state, beliefs, depth, temp)\n",
    "    \n",
    "    # Sample agent's action\n",
    "    actions, probs = zip(*actionProbs)\n",
    "    prediction = rand.choice(range(len(actions)), p = probs)\n",
    "    \n",
    "    return actions[prediction][agent]\n",
    "\n",
    "# Samples the actions of all agents in a single turn (useful for simulation)\n",
    "def combinedModel(state, beliefs, depth = 2, temp = 1):\n",
    "    n = len(beliefs)\n",
    "    return [agentModel(state, beliefs, depth, agent, temp) for agent in range(n)]\n",
    "\n",
    "def updateBeliefs(state, beliefs, actions, depth, temp, alpha):\n",
    "    actionProbs = actionPrediction(state, beliefs, depth, temp)\n",
    "    expectation = marginalize(beliefs, actionProbs)\n",
    "    \n",
    "    actionsToBinary = [1 if a == 'tree' else 0 for a in actions]\n",
    "    \n",
    "    predictionErr = np.multiply(alpha, np.subtract(actionsToBinary, expectation))\n",
    "    \n",
    "    return np.add(beliefs, predictionErr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Putting it all together\n",
    "\n",
    "### Counterbalancing\n",
    "\n",
    "As a first pass, we'll compare model performance to human performance in the simplest possible way—by making the model do the same task! The code below creates a set of test scenarios to use on the model. These are structured in the same way the task itself was structured. \n",
    "\n",
    "In particular, there are three possible types of scenarios: scenarios with three solutions (such as `{'strengths': [1, 1, 1], 'trees': 1}`), scenarios with two solutions (e.g., `{'strengths': [1, 2, 1], 'trees': 3`), and scenarios with one solution (e.g, `{'strengths': [1, 3, 3], 'trees': 1}`). In each run of the task, the model will be tested on 8 scenarios from each category. The scenarios are ordered in blocks of 3 scenarios, such that each block has scenarios from all categories in a random order (e.g., (A B C) (C A B) (B C A) etc.).\n",
    "\n",
    "But it's probably simpler to see the output of the function, below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'n_solutions': 3, 'strengths': (3, 3, 3), 'num': 1, 'trees': 3},\n",
       "       {'n_solutions': 1, 'strengths': (3, 3, 1), 'num': 2, 'trees': 1},\n",
       "       {'n_solutions': 2, 'strengths': (3, 1, 1), 'num': 3, 'trees': 1},\n",
       "       {'n_solutions': 3, 'strengths': (1, 1, 1), 'num': 4, 'trees': 2},\n",
       "       {'n_solutions': 1, 'strengths': (2, 1, 3), 'num': 5, 'trees': 1},\n",
       "       {'n_solutions': 2, 'strengths': (3, 2, 2), 'num': 6, 'trees': 2},\n",
       "       {'n_solutions': 1, 'strengths': (2, 2, 3), 'num': 7, 'trees': 3},\n",
       "       {'n_solutions': 3, 'strengths': (2, 2, 2), 'num': 8, 'trees': 3},\n",
       "       {'n_solutions': 2, 'strengths': (2, 2, 1), 'num': 9, 'trees': 3},\n",
       "       {'n_solutions': 3, 'strengths': (1, 1, 1), 'num': 10, 'trees': 1},\n",
       "       {'n_solutions': 2, 'strengths': (3, 2, 1), 'num': 11, 'trees': 3},\n",
       "       {'n_solutions': 1, 'strengths': (3, 2, 2), 'num': 12, 'trees': 3},\n",
       "       {'n_solutions': 1, 'strengths': (1, 1, 3), 'num': 13, 'trees': 3},\n",
       "       {'n_solutions': 3, 'strengths': (3, 3, 3), 'num': 14, 'trees': 1},\n",
       "       {'n_solutions': 2, 'strengths': (2, 1, 3), 'num': 15, 'trees': 3},\n",
       "       {'n_solutions': 1, 'strengths': (2, 1, 3), 'num': 16, 'trees': 2},\n",
       "       {'n_solutions': 3, 'strengths': (3, 3, 3), 'num': 17, 'trees': 2},\n",
       "       {'n_solutions': 2, 'strengths': (2, 1, 1), 'num': 18, 'trees': 3},\n",
       "       {'n_solutions': 3, 'strengths': (2, 2, 2), 'num': 19, 'trees': 2},\n",
       "       {'n_solutions': 1, 'strengths': (2, 3, 1), 'num': 20, 'trees': 2},\n",
       "       {'n_solutions': 2, 'strengths': (3, 3, 2), 'num': 21, 'trees': 3},\n",
       "       {'n_solutions': 2, 'strengths': (1, 3, 1), 'num': 22, 'trees': 1},\n",
       "       {'n_solutions': 1, 'strengths': (1, 3, 2), 'num': 23, 'trees': 2},\n",
       "       {'n_solutions': 3, 'strengths': (2, 2, 2), 'num': 24, 'trees': 1}], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def counterbalancer(n):\n",
    "    # Get all possible states\n",
    "    allTrees = range(1,n+1)\n",
    "    allStrengths = list(itertools.product(allTrees, repeat = 3))\n",
    "    allStates = list(itertools.product(allStrengths, allTrees))\n",
    "    allStates = [{'strengths': s, 'trees': t} for s, t in allStates] # Convert to dictionary\n",
    "\n",
    "    for state in allStates:\n",
    "        payoffs = getAllRewards(state)\n",
    "        state['n_solutions'] = np.count_nonzero(payoffs)\n",
    "\n",
    "    # Remove states with no solutions\n",
    "    allStates = [state for state in allStates if state['n_solutions'] > 0]\n",
    "\n",
    "    # Group all states according to the number of possible solutions\n",
    "    state_groups = np.unique([state['n_solutions'] for state in allStates])\n",
    "    statesByGroup = [[state for state in allStates if state['n_solutions'] == group] for group in state_groups]\n",
    "\n",
    "    # Sample 8 states from each group\n",
    "    randomSubset = [rand.choice(group, size = 8, replace = False) for group in statesByGroup]\n",
    "\n",
    "    # Arrange into blocks and randomize within block\n",
    "    trialOrder = zip(*randomSubset)\n",
    "    trialOrder = [rand.permutation(block) for block in trialOrder]\n",
    "    trialOrder = np.vstack(trialOrder).reshape(-1) # Flattened array\n",
    "    \n",
    "    # Add trial number\n",
    "    for t in range(len(trialOrder)):\n",
    "        trialOrder[t]['num'] = t+1\n",
    "    \n",
    "    return trialOrder\n",
    "\n",
    "# Example output\n",
    "counterbalancer(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Running the simulation\n",
    "\n",
    "The moment we've all been waiting for! The function `simulate` puts everything together and:\n",
    "\n",
    "1. Creates a new trial order for each simulation\n",
    "2. Passes the model through each trial and receives each agent's response\n",
    "3. Makes agents try again if they don't get any reward\n",
    "4. Updates prior on each agent's actions\n",
    "5. Passes the new priors on to the next trial\n",
    "6. Returns the model's trial history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simulate(n, depth, temp, alpha):\n",
    "    trialOrder = counterbalancer(n)\n",
    "    beliefs = [.5]*n\n",
    "\n",
    "    trialHistory = []\n",
    "    for trial in trialOrder:\n",
    "        while True:\n",
    "            actions = combinedModel(trial, beliefs, depth, temp)\n",
    "            payoff = getReward(trial, actions)\n",
    "            beliefs = updateBeliefs(trial, beliefs, actions, depth, temp, alpha)\n",
    "\n",
    "            trialInfo = dict(trial, **{'actions': actions, \\\n",
    "                                       'beliefs': list(beliefs), \\\n",
    "                                       'payoff': payoff, \\\n",
    "                                       'is_max': payoff > 0})\n",
    "            trialHistory.append(trialInfo)\n",
    "\n",
    "            if payoff > 0:\n",
    "                break\n",
    "    \n",
    "    return trialHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing simulation\n",
    "\n",
    "Now that the simulator is ready, we're going to run many, many simulations. (This code takes a little over a minute to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "n = 3\n",
    "depth = 3\n",
    "temp = 2.5\n",
    "alpha = 0.1\n",
    "\n",
    "# Simulation parameters\n",
    "n_iterations = 1000\n",
    "\n",
    "# Let's go!\n",
    "startTime = time.clock()\n",
    "allSimulations = [simulate(n, depth, temp, alpha) for i in range(n_iterations)]\n",
    "endTime = time.clock()\n",
    "print 'Simulation completed in %0.2f seconds' % (endTime - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll save the output of the simulation as a CSV file and analyze it in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simulationData = []\n",
    "\n",
    "# Change into dataset-friendly format (e.g., by splitting lists into multiple columns)\n",
    "for iteration in range(n_iterations):\n",
    "    for trial in range(len(allSimulations[iteration])):\n",
    "        trialInfo = allSimulations[iteration][trial]\n",
    "        newData = dict(trialInfo, **{'iteration': iteration+1, \\\n",
    "                                     'p1_prior': trialInfo['beliefs'][0], \\\n",
    "                                     'p2_prior': trialInfo['beliefs'][1], \\\n",
    "                                     'p3_prior': trialInfo['beliefs'][2], \\\n",
    "                                     'p1_strength': trialInfo['strengths'][0], \\\n",
    "                                     'p2_strength': trialInfo['strengths'][1], \\\n",
    "                                     'p3_strength': trialInfo['strengths'][2], \\\n",
    "                                     'p1_action': trialInfo['actions'][0], \\\n",
    "                                     'p2_action': trialInfo['actions'][1], \\\n",
    "                                     'p3_action': trialInfo['actions'][2], \\\n",
    "                                    })\n",
    "        \n",
    "        # Remove list variables\n",
    "        del newData['beliefs']\n",
    "        del newData['actions']\n",
    "        del newData['strengths']\n",
    "        \n",
    "        simulationData.append(newData)\n",
    "\n",
    "# Change list of dictionaries into a dictionary of lists, and then into a pandas dataframe\n",
    "simulationData = {key:[item[key] for item in simulationData] for key in simulationData[0].keys()}\n",
    "simulationData = pd.DataFrame(simulationData)\n",
    "\n",
    "# Save simulation parameters to separate file\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "simulationParams = {'n': n, 'depth': depth, 'temp': temp, 'alpha': alpha, 'n_iterations': n_iterations}\n",
    "\n",
    "simulationData.to_csv('simulation_results_%s.csv' % (timestamp))\n",
    "\n",
    "with open('simulation_params_%s.json' % (timestamp), 'w') as f:\n",
    "    json.dump(simulationParams, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Debugging: testing this model's predictions against Kelsey's original fisherman model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.749912724973\n",
      "[0.69126374554776404, 0.17655879940547115, 0.17655879940547115]\n"
     ]
    }
   ],
   "source": [
    "import action_prediction_clean as ka\n",
    "\n",
    "R = ka.getRewards_multi([1,1,1],1)\n",
    "print ka.QRE_multi(R,2,2.5, np.array([.55, .5, .5]))\n",
    "\n",
    "state = {'trees': 1, 'strengths': [1, 1, 1]}\n",
    "beliefs = [.55, .5, .5]\n",
    "probs = actionPrediction(state, beliefs, 2, 2.5)\n",
    "print marginalize(beliefs, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
